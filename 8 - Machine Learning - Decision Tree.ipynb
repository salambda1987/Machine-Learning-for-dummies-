{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Python Machine Learning 8\n",
    "# Decision Tree\n",
    "## *By Sal Lascano*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! here is a very elementary decision tree. If a person likes The Beatles, they are awesome, if they dont then they are slightly less than awesome. In general a decision tree asks a question and then classifies the person based on the answer. The example i mentioned is based on a yes/no answer. \n",
    "\n",
    "We can use a decision tree on ranked data as well. For example lets say we have 3 different outcomes in how a person is feeling at the moment: Hungry, Neutral or Full. If a person is hungry they need to eat, if they are neutral then perhaps they need a snack and if they are full then they dont need to eat at all. \n",
    "\n",
    "The classification can be cathegories or numeric and given a tree with many nodes it is most likely that it will be a combination of both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classification in a desicion tree is perhaps the most important, its called the Root. The order of the other nodes depends on how well the feature descrives the target variable. We can find out how well the feature descrived the target by using gini impurity. The less impure the feaute is, the better is descrives the target.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree in Scikit Learn\n",
    "\n",
    "### Arguments:\n",
    "\n",
    "**criterion**: \"gini\" or \"entropy\", corresponding to the criteria of \"gini impurity\" and \"information gain\". default = 'gini'.\n",
    "\n",
    "**max_depth**: The maximum depth of the tree. default = None, which means the nodes will be expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "**min_samples_split**: The minimum number of samples required to split. default = 2.\n",
    "\n",
    "**min_samples_leaf **: The minimum number of samples required to be at a terminate node. default = 1.\n",
    "\n",
    "### Methods:\n",
    "\n",
    "**fit**: Build a decision tree from the training set (X, y).\n",
    "\n",
    "**predict**: \tPredict class or regression value for X.\n",
    "\n",
    "**predict_log_proba**\tPredict class log-probabilities of the input samples X.\n",
    "\n",
    "**predict_proba**\tPredict class probabilities of the input samples X.\n",
    "\n",
    "**score**:\tReturn the mean accuracy on the given test data and labels.\n",
    "\n",
    "**set_params**:\tSet the parameters of this estimator.\n",
    "\n",
    "**get_params**: Get parameters for this estimator.\n",
    "\n",
    "### Attributes:\n",
    "\n",
    "**tree\\_**: Tree object, the underlying tree object.\n",
    "\n",
    "**feature\\_importances\\_**: The feature importances. The higher, the more important the feature. Also known as gini importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on a hands on example to get more familiar with decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets import the needed libraries to start this exercise\n",
    "from sklearn import tree\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "from sklearn import datasets\n",
    "\n",
    "#In this example we will use our beloved iris data set\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#Lets use ony two features of the fiur that this data set has to fit the model. If we use all four of them then\n",
    "#The prediction would be perfect, but i do still believe that with two, the model will perform very well.\n",
    "#Lets fit the model.\n",
    "tree_model.fit(iris.data[:, 2:4], iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets find out the accuracy of the model\n",
    "tree_model.score(iris.data[:, 2:4], iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model does extremely well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06492158 0.93507842]\n"
     ]
    }
   ],
   "source": [
    "#Lets see how important the feautes are\n",
    "print(tree_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the second feature 'petal width' is far more important than the first 'petal lenght' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to know that as the depth of a tree increases, the training error goes down but the test error does not change much hence it is better to chose a small tree. If the outcome doesnt change then we want to have a tree as cost effective as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decition tree pros\n",
    "\n",
    "Decision trees are very easy to interpret. Easy to figure out important features.\n",
    "\n",
    "### Decition tree cons\n",
    "\n",
    "They can be unstable, a very small change in the data can turn into a huge change in the outcome. The accuracy is not always the best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
