{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning 5\n",
    "\n",
    "# Feature Selection / PCA\n",
    "\n",
    "## By Sal Lascano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with, **Why bother with Dimensionality reduction?** The reason we do dimensionality reduction is because removing irrelevant features results in a better performing algorithm/more accurate and it will obviously run faster.\n",
    "\n",
    "Machine Learing Algorithms are only as good as the features we provide (Garbage in, garbage out). Coming up with good features is one of the **most important jobs in machine learning** choosing the right number of features to use is more of an art than a science. Now lets discuss what makes a good feature. \n",
    "\n",
    "A good feature makes it easier for the algorithm to decide between different things, a feature that provides plenty information to the algorith or statistically speaking a feature rich in variance is one of those (but not filled with outliers)\n",
    "\n",
    "We want our features to be independent. We dont want features that are correlated or redundant to one and other. If such case comes across pick the one that seems most useful and disregard the other ones. Algorithms are not smart enough to realize that for example height in inches and centimiters are the same so they will double count the importance of the feature. \n",
    "\n",
    "Again we want to help our algorith be as accurate as possible. So we want features that are easy to read and interpret by the algorith. For example if we want to know how long a letter will take from place a to get to place b, a good easy to read feaure will be one that gives the distance between these two cities in miles. A hard to read feature will give us the coordenates of the two cities. Pretty much the same information but handed over in different formats where in the second example it is harder for the algorith to get the essence of the feature.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, the goals of feature selection are:\n",
    "    - Improving the prediction performance\n",
    "    - Providing faster and more cost-effective predictors\n",
    "    - Providing a better understanding of the underlying process that generated the data\n",
    "  \n",
    "In this notebook we will introduce some basic methods from a mathematic perspective, Lets start. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing features with low variance\n",
    "\n",
    "If a feature has the same value in all observations, then the feature is telling us nothing so its useless. In this cases the **VarianceThreshold** is used to remove all these useless features. As its name states, the **VarianceThreshold** removes features whos variance does not meet a stated threshold. By default it deletes all features with 0 variance. \n",
    "\n",
    "Lets see an example:\n",
    "\n",
    "In this example we will work with the famous iris data set wish has four features: sepal length, sepal width, petal length, petal width. Lets see the variance of these features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (150, 4)\n",
      "Variation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'petal length (cm)': 3.0924248888888854,\n",
       " 'petal width (cm)': 0.5785315555555559,\n",
       " 'sepal length (cm)': 0.6811222222222222,\n",
       " 'sepal width (cm)': 0.1867506666666667}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import all the needed libraries and the dataset \n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#printthe shape of the data set, we have 150 observations and 4 features\n",
    "print('Shape: (%d, %d)' %iris.data.shape)\n",
    "\n",
    "#print title\n",
    "print('Variance:')\n",
    "\n",
    "#Prinitng the variance of all 4 features\n",
    "dict(zip(iris.feature_names, np.var(iris.data, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets say we want to transform this numpy array into a DataFrame, we use pandas for that.\n",
    "irisDF = pd.DataFrame(data=iris.data[0:,0:], columns=iris.feature_names)\n",
    "\n",
    "#See the first 5 rows using head()\n",
    "irisDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets remove all features with a variance less than 0.6, we should be left off with 2 features according to the above information, lets see.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets import feature selection from sklearn\n",
    "import sklearn.feature_selection as fs\n",
    "\n",
    "#Now set the threshold, the argument \"threshold= 0.6\" is used to define the threshold.\n",
    "x_new = fs.VarianceThreshold(threshold = 0.6).fit_transform(iris.data)\n",
    "\n",
    "#After the selection, the new variable  x_new has only 2 predictors.\n",
    "x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Feature Selection\n",
    "\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests.\n",
    "\n",
    "#### SelectKBest\n",
    "\n",
    "SelectKBest is used to keep the $k$ highest scoring features. It uses $\\chi^2$ test to pick the best features. Lets see how it works on the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape\n",
      "(150, 2)\n",
      "   \n",
      "Slice first 5 rows of our new variable best2 with the best 2 features only\n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]]\n",
      "   \n",
      "Slice first 5 rows of the iris data with all original features\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# In fs.SelectKBest, fs.chi2 means we are going to use chi-square and k=2 means we will select the best 2 features\n",
    "# In fit_transform, we should enter the independent and the dependent variables\n",
    "best2 = fs.SelectKBest(fs.chi2, k=2).fit_transform(iris.data, iris.target)\n",
    "\n",
    "print('Shape')\n",
    "print(best2.shape)\n",
    "print('   ')\n",
    "print('Slice first 5 rows of our new variable best2 with the best 2 features only')\n",
    "print(best2[:5,])\n",
    "print('   ')\n",
    "print('Slice first 5 rows of the iris data with all original features')\n",
    "print(iris.data[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectPercentile \n",
    "\n",
    "SelectPercentile is used when there are thousands of features. It works same as SelectKBest using the $\\chi^2$ test to pick the best features just that here we are asking for a percentile not an actual number. lets get to the code.\n",
    "\n",
    "we will ask for the top 50% so the results should be the exact same as previous example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape\n",
      "(150, 2)\n",
      "   \n",
      "Slice first 5 rows of our new variable percent50 with the top 50% features only\n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]]\n",
      "   \n",
      "Slice first 5 rows of the iris data with all original features\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "   \n",
      "It worked just as expected YEY\n"
     ]
    }
   ],
   "source": [
    "# In fs.SelectKBest, fs.chi2 means we are going to use chi-square and the 50 means we will select \n",
    "# the top 50% features\n",
    "# In fit_transform, we should enter the independent and the dependent variables\n",
    "percent50 = fs.SelectPercentile(fs.chi2, 50).fit_transform(iris.data, iris.target) \n",
    "\n",
    "\n",
    "print('Shape')\n",
    "print(percent50.shape)\n",
    "print('   ')\n",
    "print('Slice first 5 rows of our new variable percent50 with the top 50% features only')\n",
    "print(percent50[:5,])\n",
    "print('   ')\n",
    "print('Slice first 5 rows of the iris data with all original features')\n",
    "print(iris.data[:5,])\n",
    "print('   ')\n",
    "print('It worked just as expected YEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Regression we use the f_regression function to perform an F-test. We will use the iris data set to do a little exercise. Lets say we want to predict the fourth feature based on all four features, we will ask for the best feature to predict the fourth which in essence will be its self. Lets see the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice first 5 rows of our new variable best1 with the feature that explains better the target (its self)\n",
      "[[0.2]\n",
      " [0.2]\n",
      " [0.2]\n",
      " [0.2]\n",
      " [0.2]]\n",
      "   \n",
      "Slice first 5 rows of the iris data with all original features\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sals/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:303: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n"
     ]
    }
   ],
   "source": [
    "# Set all 4 features as our independent variables \n",
    "iris.x = iris.data[:,:]\n",
    "\n",
    "# Set out fourth feature as our dependent variable\n",
    "iris.y = iris.data[:, 3]\n",
    "\n",
    "# In fs.SelectKBest, fs.f_regression means we are useing the F-Test and k=1 means we will select the best feature\n",
    "# In fit_transform, we should enter the independent and the dependent variables\n",
    "best1 = fs.SelectKBest(fs.f_regression, k=1).fit_transform(iris.x, iris.y)\n",
    "\n",
    "print('Slice first 5 rows of our new variable best1 with the feature that explains better the target (its self)')\n",
    "print(best1[:5, :])\n",
    "print('   ')\n",
    "print('Slice first 5 rows of the iris data with all original features')\n",
    "print(iris.x[:5, :])\n",
    "print('   ')\n",
    "print('Again, it worked just as expected, it chose its self')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
