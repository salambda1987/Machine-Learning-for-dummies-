{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning 4\n",
    "\n",
    "# Model Validation\n",
    "\n",
    "## *By Sal lascano*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Data Scientists, we will very often come across a dangerous problem. To think that a model will perform very well, but when in production it is not performing as spected or desired. In the lucky cases it can just be a massive waste of time but it becomes dangerous when large amounts of money or even human lifes are of the line. \n",
    "\n",
    "There are many things that could affect a model to not perform well but most of the time it is that the model was not validated correctly. A wrong validation can give us over-optimistic expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training error and Test error\n",
    "\n",
    "Regardless of the model, we as Data Scientist must always want to meassure how accurate it is. We do this to be able to select the best performing model and its parametes so that the model becomes more accurate. \n",
    "\n",
    "The basic way to calculate the accuracy of a model is to *train* the model on a given data set and then use it on data where we already know the dependent variable $y$. We then compare the values of $y$ with the values that the model predicted, this is called the **Classification error**. to get the Classification error we count the number of times the outcome of the model differed from the actual values of $y$ and divide this by the number of rows in the data set. \n",
    "\n",
    "There are two very important concepts in Machine Learning:\n",
    "\n",
    "### - Training error\n",
    "It is the classification error we get from applying the model to the same data it was trained on. Hence we are using the same data set to train and test the model. \n",
    "\n",
    "### - Test error\n",
    "It is the classification error we get from appying the model to a data set the model has not seen yet. Hence we are using 2 different data sets, one for training and another for testing. ofter refered as the prediction error\n",
    "\n",
    "#### Training error often underestimates Test error\n",
    "\n",
    "The goal of machine learning is to minimize the error in future predictions, in the best casenario we will have both training and testing sets but that is not always available. \n",
    "\n",
    "### Validation\n",
    "\n",
    "Tipically what we will do is divide our data set into two parts: Training set and Test set/Validation set. If the data is not big enough then **Cross validation** is your best answer (To be discussed next in this notebook) but if your data set is generous say 100,000 observarions, then rule-of-thumb is to divide Taining set: 80% and Test set: 20%.\n",
    "\n",
    "We will fit the model to the Training test and then see how it performs on the Test set. \n",
    "\n",
    "- For quantitative response/regression we are dealing with the **MSE** (Mean squared error)\n",
    "- For categorical response/classificarion we are dealing with the **Missclassification rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "**Difficulties splitting the data into Training & Testing Sets -** The problem of splitting the data into Training and Testing sets is that we want to maximize both of those sets. We want the maximum amout of data in our Training set to get the best result and the maximum amout of data in our Testing set to get the best validation. By splitting the data into 2 sets, every data point taken from the Training set to the Test set it is lost to the Training set and vice versa. \n",
    "\n",
    "What C.V. (Cross Validation) does is, instead of splitting the data into 2 sets, it splits the data into $K$  equal sized parts (*$K$ Folds*). Then picks one of these parts $k$ (To be used as the Test set) and trains the rest of the data or $K - k$/$K - 1$. It uses $k$ as the Test set to be able to get the classification error for this validation. This procedure is done in order to all $k$ = 1, 2, ... $K$. For example if we were to split the data into 10 *Folds* we would have 10 classification errors which will be averaged to get the final Test error. This procedure is called $K$-Fold Cross Validation and it is very popular among Data Scientists.\n",
    "\n",
    "**Cross Validation For Regression**\n",
    "\n",
    "After dividing the data into $K$ parts, let $C_i$ denotes the $i$th part of the observations. Let $n_i$ denotes the number of the observations in $C_i$, $n$ denotes the total amount of the observations. If all the parts are equal-sized, then $n_i = \\frac{n}{k}$ for $i=1, 2, 3, ... k$.\n",
    "\n",
    "The final test error is:\n",
    "$$CV_k = \\sum_{i=1}^{k}\\frac{n_i}{n} MSE_i$$\n",
    "\n",
    "**Cross Validation For Classification**\n",
    "\n",
    "After dividing the data into $K$ parts, let $C_i$ denotes the $i$th part of the observations. Let $n_i$ denotes the number of the observations in $C_i$, $n$ denotes the total amount of the observations. If all the parts are equal-sized, then $n_i = \\frac{n}{k}$ for $i=1, 2, 3, ... k$.\n",
    "\n",
    "The final test error is:\n",
    "$$\n",
    "\\text{ }\n",
    "CV_k = \\sum_{i=1}^{k}\\frac{n_i}{n} Error_i \\quad\n",
    "\\text{ }\n",
    "$$\n",
    "\n",
    "**Leave One Out Cross Validation/LOOCV**\n",
    "\n",
    "When the data set it is too small and the amount of observations is the same as the amount $K$-*Folds* $k$ = $n$ we use LOOVC. In LOOCV each $k$ is a single observation to be tested after training the rest of observations. Normally LOOCV tends to be too expensive so rarelly used, In practice $k$ = 5 - $k$ = 10 is commonly used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation using Scikit-Learn\n",
    "\n",
    "### KFold\n",
    "\n",
    "Divides the data into $k$ parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data: [0 1 2 3 4 5]\n",
      "Train: [2 3 4 5] Validation: [0 1]\n",
      "Train: [0 1 4 5] Validation: [2 3]\n",
      "Train: [0 1 2 3] Validation: [4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.model_selection as ms\n",
    "# divide 6 samples into 3 parts\n",
    "sample = np.array(range(6))\n",
    "print('All data: {}'.format(sample))\n",
    "ms_k3 = ms.KFold(n_splits=3)\n",
    "for train_idx, val_idx in ms_k3.split(sample): \n",
    "    print('Train:', train_idx, 'Validation:', val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [1 1 1 1] Validation: [0 0]\n",
      "Train: [0 0 1 1] Validation: [1 1]\n",
      "Train: [0 0 1 1] Validation: [1 1]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0, 0, 1, 1, 1, 1])\n",
    "for train_idx, val_idx in ms_k3.split(y):\n",
    "    print('Train:', y[train_idx], 'Validation:', y[val_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StratifiedKFold\n",
    "\n",
    "We can see that in the last example for the KFold function where we used the 1's and 0's, the distribution of the validation is very variable. In the first fold, all the 0's are in the Test set and in the las 2 folds we see that all the 0's are in the Training sets. \n",
    "\n",
    "StratifiedKFold is a variation of the KFold function. It returns stratified folds where each set contains approximately the same persentage of samples of each target as the complete set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0 0 1 1 1 1] Validation:  [0 1 1]\n",
      "Train: [0 0 1 1 1 1] Validation:  [0 1 1]\n",
      "Train: [0 0 1 1 1 1] Validation:  [0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Note that providing y is sufficient to generate the splits and hence np.zeros(n_samples)\n",
    "#  may be used as a placeholder for X instead of actual training data.\n",
    "x = np.zeros(9)\n",
    "y = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "ms_k3s = ms.StratifiedKFold(n_splits=3)\n",
    "for train_idx, val_idx in ms_k3s.split(x, y):\n",
    "    print('Train:', y[train_idx], 'Validation: ', y[val_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train-Test-Split\n",
    "\n",
    "The last two functions just retun the indices of the training sets and the validation set. in **Scikit-Learn** the function **train_test_split** can be used to generate a random split into Training and Testing sets more straight forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (150, 4), (150,)\n",
      "Training: (100, 4), (100,)\n",
      "Test: (50, 4), (50,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "x_train, x_test, y_train, y_test = ms.train_test_split(iris.data, iris.target, \n",
    "                                                       test_size=1/3)\n",
    "print('Original: {}, {}'.format(iris.data.shape, iris.target.shape))\n",
    "print('Training: {}, {}'.format(x_train.shape, y_train.shape))\n",
    "print('Test: {}, {}'.format(x_test.shape, y_test.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
